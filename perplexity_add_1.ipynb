{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import collections\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTokens(file):\n",
    "    tokens=[]\n",
    "    #with open('/home/meghana/nltk_data/corpora/gutenberg/training_corpus.txt') as f:\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                line=line.lower()\n",
    "                line=line.replace('.',' ').replace(',',' ').replace(':',' ').replace(';',' ').replace('!',' ').replace('?',' ').replace('(',' ').replace(')',' ').replace(\"-\",\" \").replace(\"' \",' ').replace('\"',' ').replace(\"_\",\" \").replace('[',' ').replace(']',\" \").replace('{',' ').replace('}',\" \").replace('*',' ')   \n",
    "            items=line.split()\n",
    "            tokens+=items\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_1(table,test_set):\n",
    "    for word in table:\n",
    "        table[word]+=1\n",
    "\n",
    "    for word in test_set:\n",
    "        if word not in table:\n",
    "            table[word]=1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth(table,table_2,V,k):\n",
    "    for word in table:\n",
    "        table[word]=(table[word]/(table_2[word[0:k]]+V))\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to find perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_perplexity(table,table_2,test_set,n,k):\n",
    "    pp=1.0\n",
    "    for word in test_set:\n",
    "        pp=pp*((1/float(table[word])*table_2[word[0:k]])**(1./n))\n",
    "        #pp=pp*((1/float(table[word]))**(1/n))\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main driver functon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of tokens and vocabulary in training corpus is: 1892387 35627\n",
      "Total no of tokens and vocabulary in testing corpus is: 208016 19283\n",
      "1.0279763468053185 3.386774577336738\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    tokens=getTokens('training_corpus.txt')\n",
    "    test_tokens=getTokens('testing_corpus.txt')\n",
    "    tokenSet=set(tokens)\n",
    "    test_tokenSet=set(test_tokens)\n",
    "\n",
    "    print(\"Total no of tokens and vocabulary in training corpus is:\",len(tokens),len(tokenSet))\n",
    "    print(\"Total no of tokens and vocabulary in testing corpus is:\",len(test_tokens),len(test_tokenSet))\n",
    "\n",
    "    bigrams=list(ngrams(tokens,2))\n",
    "    trigrams=list(ngrams(tokens,3))\n",
    "    quadgrams=list(ngrams(tokens,4))\n",
    "    \n",
    "    test_bigrams=list(ngrams(test_tokens,2))\n",
    "    test_bigramSet=set(test_bigrams)\n",
    "    \n",
    "    test_trigrams=list(ngrams(test_tokens,3))\n",
    "    test_trigramSet=set(test_trigrams)\n",
    "    \n",
    "    test_quadgrams=list(ngrams(test_tokens,4))\n",
    "    test_quadgramSet=set(test_quadgrams)\n",
    "    \n",
    "    quadgram_table=collections.Counter(quadgrams)\n",
    "    trigram_table=collections.Counter(trigrams)\n",
    "    bigram_table=collections.Counter(bigrams)\n",
    "    \n",
    "    quadgram_table=add_1(quadgram_table,test_quadgramSet)\n",
    "    trigram_table=add_1(trigram_table,test_trigramSet)\n",
    "    bigram_table=add_1(bigram_table,test_bigramSet)\n",
    "    \n",
    "    #V=len(tokenSet)\n",
    "    total_tokens=tokens+test_tokens\n",
    "    V=len(set(total_tokens))\n",
    "    \n",
    "    quadgram_table=smooth(quadgram_table,trigram_table,V,3)\n",
    "    trigram_table=smooth(trigram_table,bigram_table,V,2)\n",
    "    \n",
    "    n=len(tokens)\n",
    "    \n",
    "    pp1=find_perplexity(quadgram_table,trigram_table,test_quadgramSet,n,3)\n",
    "    pp2=find_perplexity(trigram_table,bigram_table,test_trigramSet,n,2)\n",
    "    \n",
    "    print(pp1,pp2)\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
